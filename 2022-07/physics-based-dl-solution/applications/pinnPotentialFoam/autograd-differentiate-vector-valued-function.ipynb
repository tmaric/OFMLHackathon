{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed0d479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u =  tensor([1., 2., 3.], requires_grad=True)\n",
      "psi =  tensor([ 2.,  4.,  6., 14.], grad_fn=<CopySlices>)\n",
      "tensor([2., 0., 0.])\n",
      "tensor([0., 2., 0.])\n",
      "tensor([0., 0., 2.])\n",
      "tensor(6.)\n",
      "tensor([2., 4., 6.], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m d_phi_du\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(d_phi_du)\n\u001b[0;32m---> 33\u001b[0m dd_phi_du \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_phi_du\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# u = grad(Phi) = [2*u0, 2*u1, 2*u2]\n",
    "# Phi = u0**2 + u1**2 + u2**2 = dot(u,u)\n",
    "\n",
    "u = torch.tensor([1.,2.,3.], requires_grad=True)\n",
    "\n",
    "psi = torch.zeros(4)\n",
    "psi[0] = 2*u[0]\n",
    "psi[1] = 2*u[1]\n",
    "psi[2] = 2*u[2]\n",
    "psi[3] = torch.dot(u,u)\n",
    "\n",
    "print(\"u = \",u)\n",
    "print(\"psi = \",psi)\n",
    "\n",
    "grad_v_x = torch.autograd.grad(psi[0], u, retain_graph=True)[0]\n",
    "print(grad_v_x)\n",
    "grad_v_y = torch.autograd.grad(psi[1], u, retain_graph=True)[0]\n",
    "print(grad_v_y)\n",
    "grad_v_z = torch.autograd.grad(psi[2], u, retain_graph=True)[0]\n",
    "print(grad_v_z)\n",
    "\n",
    "div_v = grad_v_x[0] + grad_v_y[1] + grad_v_z[2]\n",
    "\n",
    "# Divergence of the vector phi[0:3]=2u0 + 2u1 + 2u2 w.r.t [u0,u1,u2] = 2+2+2=6\n",
    "print (div_v)\n",
    "\n",
    "# laplace(psi[3]) = \\partial_u0^2 psi[3] + \\partial_u1^2 psi[3] + \\partial_u2^2 psi[3]\n",
    "# = \\partial_u0 2x + \\partial_u1 2u1 + \\partial_u2 2u2 = 2 + 2 + 2 = 6\n",
    "d_phi_du = torch.autograd.grad(psi[3], u, retain_graph=True)[0]\n",
    "d_phi_du.requires_grad_(True)\n",
    "print(d_phi_du)\n",
    "dd_phi_du = torch.autograd.grad(d_phi_du[0], u, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd34f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
